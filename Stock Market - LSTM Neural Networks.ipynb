{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31a123c",
   "metadata": {},
   "source": [
    "# Stock Market Predictions - LSTM Neural Networks\n",
    "\n",
    "###### Ignacio Antequera Sanchez\n",
    "\n",
    "# 0. Introduction\n",
    "---\n",
    "\n",
    "Hello Everyone! \n",
    "\n",
    "Welcome to my Stock Market Predictions Project using ```LSTM Neural Networks```. My name is Ignacio Antequera and in this notebook, I will be sharing with you how we can use a time-series model known as Long Short-Term Memory.\n",
    "\n",
    "The logic behind this project is to use machine learning models that can look at the history of a sequence of data and correctly predict what the future elements of the sequence are going to be. This is where ```time series modeling``` comes in.\n",
    "\n",
    "> Disclaimer: This project is solely for learning purposes and does not serve as a financial advisor. Stock market prices are highly unpredictable and volatile. This means that there are no consistent patterns in the data that allow you to model stock prices over time near-perfectly.\n",
    "\n",
    "The objective of this project id to see if we can model the data, so that the predictions we make correlate with the actual behavior of the data. In other words, we don't need the exact stock values of the future, but the stock price movements\n",
    "\n",
    "### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8354e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "\n",
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For processing\n",
    "import math\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "import urllib.request, json\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "# Libraries for model training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ada73",
   "metadata": {},
   "source": [
    "# 1. Downloading the data\n",
    "---\n",
    "\n",
    "There are two ways we can access data for this project:\n",
    "\n",
    "- `Alpha Vnatage Stock API`: This API contains realtime and historical stock market data. This is a reliable source for historical market data that offers a wide range of financial datasets.\n",
    "\n",
    "    - Since we going to make use of the American Airlines Stock market prices to make your predictions, we set the ticker to `\"AAL\"`. \n",
    "    - We define a `url_string`, which will return a JSON file with all the stock market data for American Airlines within the last 20 years.\n",
    "    - We define a `file_to_save`, which will be the file to which we save the data. \n",
    "    - We will use the `ticker` variable that we defined beforehand to help name this file.\n",
    "    - Next, we are going to specify a condition: if you haven't already saved data.\n",
    "    - We will go ahead and grab the data from the URL that we set in `url_string`\n",
    "    - We'll store the date, low, high, volume, close, open values to a pandas DataFrame df. \n",
    "    - We will save it to file_to_save. However, if the data is already there, we'll just load it from the CSV. (This is useful for future runs of the code)\n",
    "\n",
    "- [Kaggle](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs): This is a huge stock market dataset that contains   Historical daily prices and volumes of all U.S. stocks and ETFs. For the purposes of this demo, I will be using this option. I will be working on HP's stock since it contains a good amount of data that will serve us good for our this demo. However, you may access other demos on this repository where I will be analyzing stocks whose data has been retrieved from Alphavantage. Note that if we select this option, you will need to copy the Stocks folder in the zip file to your project home folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'kaggle' # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "    api_key = '<your API key>'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = \"AAL\"\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "            for k,v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                            float(v['4. close']),float(v['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print('Data saved to : %s'%file_to_save)        \n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "else:\n",
    "\n",
    "    # ====================== Loading Data from Kaggle ==================================\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
    "    print('Loaded data from the Kaggle repository')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bf4e0",
   "metadata": {},
   "source": [
    "# 2. Data Exploration\n",
    "---\n",
    "\n",
    "Now that we have our data properly stored, let's take a quick look at its structure. But first, since we are going to be working on time series modelling, we need to make sure that the data is sorted by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Double check the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932cd97",
   "metadata": {},
   "source": [
    "# 3. Data Visualization\n",
    "---\n",
    "\n",
    "Now, let's take a look at what sort of data we have. We are interested in data with various patterns occurring over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d91c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize = (18,9))\n",
    "\n",
    "# Create the line plot\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "\n",
    "# Set the x-axis tick marks to be every 500 rows in df\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6efc",
   "metadata": {},
   "source": [
    "This graph provides valuable insights into various aspects. The reason we chose this particular company over others is due to the diverse range of behaviors exhibited by its stock prices over time, making it an excellent candidate for robust learning and providing ample opportunities to evaluate prediction accuracy across different scenarios.\n",
    "\n",
    "It's worth noting that the stock prices around 2017 exhibit significantly higher values and greater fluctuations compared to those around the 1970s. Therefore, it's essential to ensure consistent value ranges for the data throughout the entire time frame. This normalization process will be addressed during the data normalization phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5d9a1",
   "metadata": {},
   "source": [
    "# 4. Splitting Data into Training set and Test set\n",
    "---\n",
    "\n",
    "We will use the mid price calculated by taking the average of the highest and lowest recorded prices on a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df['High'].values\n",
    "low_prices = df['Low'].values\n",
    "mid_prices = (high_prices+low_prices)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb7ed4",
   "metadata": {},
   "source": [
    "This will allow us to split the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Testing set shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c208e",
   "metadata": {},
   "source": [
    "It appears that our data has been properly split into training and testing sets. The training set contains 11,000 data points, while the testing set contains 1,075 data points. This aligns with our expectations, as the first 11,000 data points were assigned to the training set, and the remaining data points were assigned to the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4167",
   "metadata": {},
   "source": [
    "# 5. Normalizing the Data\n",
    "---\n",
    "\n",
    "Before proceeding with training our model, it's important to normalize the data to ensure that all features are on a similar scale. Normalization helps in improving the convergence speed and performance of machine learning algorithms, especially those sensitive to feature scales.\n",
    "\n",
    "Once the data is normalized, we can proceed with training your model using the training set and evaluating its performance on the normalized testing set.\n",
    "\n",
    "When scaling, we normalize both test and train data with respect to training data because we are not supposed to have access to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90fd02",
   "metadata": {},
   "source": [
    "This scaler ensures that the data is scaled to be between 0 and 1\n",
    "\n",
    "Due to the observation we made earlier, that is, different time periods of data have different value ranges, we will normalize the data by splitting the full series into windows. If we don't do this, the earlier data will be close to 0 and will not add much value to the learning process. In this case we are going to split the data into 4 windows of same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba92a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0, 10000, smoothing_window_size):\n",
    "    # Fit the scaler to the current window of training data and smooth data\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    \n",
    "    # Transform and normalize the current window of training data and smooth data\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# Normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7552c",
   "metadata": {},
   "source": [
    "Reshape the data back to the shape of [data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8523ff",
   "metadata": {},
   "source": [
    "We can now smooth the data using the exponential moving average. This helps you to get rid of the inherent raggedness of the data in stock prices and produce a smoother curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62adda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for exponential moving average (EMA) and smoothing factor (gamma)\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "\n",
    "# Perform exponential moving average smoothing on the training data\n",
    "for ti in range(11000):\n",
    "    EMA = gamma * train_data[ti] + (1 - gamma) * EMA\n",
    "    train_data[ti] = EMA\n",
    "\n",
    "# Concatenate the smoothed training data with the normalized test data for visualization and testing purposes\n",
    "all_mid_data = np.concatenate([train_data, test_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa360e2b",
   "metadata": {},
   "source": [
    "# 6. One-Step Ahead Prediction via Averaging\n",
    "---\n",
    "We can better understand the difficulty of this problem by first trying to model this as an average calculation problem. Averaging mechanisms allow us to predict (often one time step ahead) by representing the future stock price as an average of the previously observed stock prices. Doing this for more than one time step can produce quite bad results. \n",
    "\n",
    "However, we are going to look at two averaging techniques below:\n",
    "- `Standard Averaging`\n",
    "- `Exponential Moving Average`\n",
    "\n",
    "We will evaluate  the results produced by the two algorithms both qualitatively (visual inspection) and quantitatively (Mean Squared Error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e51a41",
   "metadata": {},
   "source": [
    "### Standard Averaging\n",
    "\n",
    "In this method, you will try to predict the future stock market prices as an average of the previously observed stock market prices within a fixed size window (say previous 100 days). In other words, you say the prediction at `t+1` is the average value of all the stock prices you observed within a window of `t` to `t-N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size for standard averaging\n",
    "window_size = 100\n",
    "\n",
    "# Initialize lists to store predictions, MSE errors, and prediction dates\n",
    "std_avg_predictions = []\n",
    "mse_errors = []\n",
    "std_avg_dates = []\n",
    "\n",
    "# Calculate the total number of data points in the training set\n",
    "N = len(train_data)\n",
    "\n",
    "# Iterate over the indices of the training data, starting from window_size\n",
    "for pred_idx in range(window_size, N):\n",
    "    # Calculate the prediction date\n",
    "    date = df['Date'].iloc[pred_idx]\n",
    "\n",
    "    # Calculate the standard average prediction\n",
    "    prediction = np.mean(train_data[pred_idx - window_size:pred_idx])\n",
    "    std_avg_predictions.append(prediction)\n",
    "\n",
    "    # Calculate the squared error and append it to the list of MSE errors\n",
    "    mse_errors.append((prediction - train_data[pred_idx]) ** 2)\n",
    "    std_avg_dates.append(date)\n",
    "\n",
    "# Print the MSE error for standard averaging\n",
    "print('MSE error for standard averaging: %.5f' % (0.5 * np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc9759",
   "metadata": {},
   "source": [
    "It seems like the Mean Squared Error (MSE) for standard averaging in our output is 0.00005. This indicates that the standard averaging method resulted in a relatively low MSE error, which suggests that the predictions are close to the actual values on average.\n",
    "\n",
    "A low MSE error indicates that the model's predictions are accurate and closely match the actual values. In the context of stock market prediction, a low MSE error indicates that the standard averaging method is performing well in predicting future stock prices based on historical data.\n",
    "\n",
    "---\n",
    "\n",
    "Let's Take a look at the averaged results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd71064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the true mid prices and standard averaging predictions\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "# Plot the true mid prices\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')\n",
    "\n",
    "# Plot the standard averaging predictions\n",
    "plt.plot(range(window_size, N), std_avg_predictions, color='orange', label='Prediction')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Date')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Mid Price')\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06db2f",
   "metadata": {},
   "source": [
    "The model appears to perform reasonably well for very short-term predictions (one day ahead). This is sensible since stock prices typically do not undergo drastic changes overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee9473",
   "metadata": {},
   "source": [
    "### Exponential Moving Average\n",
    "\n",
    "In the exponential moving average method, we make our predictions such that:\n",
    "\n",
    "x_{t+1} = EMA_{t} = \\gamma \\times EMA_{t-1} + (1-\\gamma) x_t\n",
    "\n",
    "where \\( EMA_0 = 0 \\) and \\( EMA \\) is the exponential moving average value maintained over time.\n",
    "\n",
    "The above equation basically calculates the exponential moving average from $t+1$ time step and uses that as the one step ahead prediction. $\\gamma$ decides what the contribution of the most recent prediction is to the EMA. For example, a $\\gamma=0.1$ gets only 10% of the current value into the EMA. Because you take only a very small fraction of the most recent, it allows to preserve much older values you saw very early in the average. See how good this looks when used to predict one-step ahead below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda90d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size for the exponential moving average\n",
    "window_size = 100\n",
    "\n",
    "# Calculate the total number of data points in the training set\n",
    "N = len(train_data)\n",
    "\n",
    "# Initialize lists to store predictions, dates, and MSE errors\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "# Initialize the running mean using the first window of data\n",
    "running_mean = np.mean(train_data[:window_size])\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "# Define the decay factor for exponential decay\n",
    "decay = 0.5\n",
    "\n",
    "# Iterate over the indices of the training data\n",
    "for pred_idx in range(1, N):\n",
    "    # Calculate the running mean using exponential decay\n",
    "    running_mean = running_mean * decay + (1.0 - decay) * train_data[pred_idx - 1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    \n",
    "    # Calculate the squared error and append it to the list of MSE errors\n",
    "    squared_error = (running_mean - train_data[pred_idx]) ** 2\n",
    "    mse_errors.append(squared_error)\n",
    "    \n",
    "    # Append date if available\n",
    "    run_avg_x.append(date)  # Make sure 'date' is defined within the loop\n",
    "\n",
    "# Calculate the MSE error for EMA averaging\n",
    "mse_error_ema = 0.5 * np.mean(mse_errors)\n",
    "print('MSE error for EMA averaging: %.5f' % mse_error_ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd62e3",
   "metadata": {},
   "source": [
    "This low MSE value suggests that the EMA method is performing well in predicting future stock prices one step ahead, based on the given historical data. A lower MSE indicates that the predictions are close to the actual values, demonstrating the effectiveness of the EMA averaging technique in this context.\n",
    "\n",
    "Now, let's visualize the results of the EMA predictions along with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a2900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the true values and predictions using Exponential Moving Average (EMA)\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')  # Plot true values\n",
    "plt.plot(range(0, N), run_avg_predictions, color='orange', label='Prediction')  # Plot EMA predictions\n",
    "\n",
    "plt.xlabel('Date')  # Set x-axis label\n",
    "plt.ylabel('Mid Price')  # Set y-axis label\n",
    "plt.legend(fontsize=18)  # Add legend with font size\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ec6fd",
   "metadata": {},
   "source": [
    "Exponential Moving Average (EMA) may seem to perform well in predicting the next day's stock market value, as evidenced by its low Mean Squared Error (MSE) and close alignment with the true distribution. However, in practical terms, knowing just the next day's value isn't particularly useful. What's more valuable is understanding whether stock prices will rise or fall over the next 30 days. Attempting to predict larger windows of time reveals the limitations of EMA.\n",
    "\n",
    "For instance, consider the scenario where $x_t=0.4$, $EMA=0.5$, and $\\gamma = 0.5$:\n",
    "- Using the equation $X_{t+1} = EMA_t = \\gamma \\times EMA_{t-1} + (1 - \\gamma)X_t$, we find $x_{t+1} = 0.5 \\times 0.5 + (1-0.5) \\times 0.4 = 0.45$. Consequently, $X_{t+1} = EMA_t = 0.45$.\n",
    "\n",
    "Applying the same logic for $X_{t+2}$: \n",
    "- we get $X_{t+2} = \\gamma \\times EMA_t + (1-\\gamma)X_{t+1}$.\n",
    "- Substituting the values, $X_{t+2} = \\gamma \\times EMA_t + (1-\\gamma) EMA_t = EMA_t = 0.45$. \n",
    "\n",
    "This means that regardless of how many steps into the future we predict, we'll always get the same answer.\n",
    "\n",
    "To address this limitation and obtain more meaningful predictions, we can explore momentum-based algorithms. These algorithms assess whether recent values have been increasing or decreasing, rather than focusing on exact values. For example, if prices have been declining in recent days, these algorithms might predict a lower price for the next day, which aligns with intuitive expectations. However, for a more sophisticated approach, we'll utilize a Long Short-Term Memory (LSTM) model.\n",
    "\n",
    "LSTM models have gained popularity in time series prediction due to their ability to effectively model temporal data. We'll investigate whether there are discernible patterns in the data that can be leveraged for more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f09c62",
   "metadata": {},
   "source": [
    "# 7. Making Stock Movement Predictions Far into the Future Using LSTM\n",
    "---\n",
    "Long Short-Term Memory models are extremely powerful time-series models. They can predict an arbitrary number of steps into the future. TensorFlow provides a nice sub API (called RNN API) for implementing time series models. We will be using that for our implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
